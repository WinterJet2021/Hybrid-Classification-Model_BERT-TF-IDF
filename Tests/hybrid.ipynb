{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3aa386",
   "metadata": {},
   "source": [
    "# Hybrid TF-IDF + BERT Interest Classification Model\n",
    "## Code ∙ Version 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ab1fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 19:59:16,282 - INFO - Loading dataset: survey_interest_dataset_enhanced.csv\n",
      "2025-04-12 19:59:16,306 - INFO - Converting labels_list from string to list...\n",
      "2025-04-12 19:59:16,315 - INFO - Initializing classifier with alpha=0.6, threshold=0.5\n",
      "2025-04-12 19:59:16,317 - INFO - Initializing BERT zero-shot classifier with model: facebook/bart-large-mnli\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tueyc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 19:59:18,856 - WARNING - From c:\\Users\\tueyc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Device set to use cpu\n",
      "2025-04-12 19:59:19,738 - INFO - BERT classifier successfully initialized\n",
      "2025-04-12 19:59:19,740 - INFO - Training the model...\n",
      "2025-04-12 19:59:19,742 - INFO - Starting model training...\n",
      "2025-04-12 19:59:19,742 - INFO - Preprocessing text data...\n",
      "2025-04-12 19:59:19,798 - INFO - Target shape: (3018, 7)\n",
      "2025-04-12 19:59:19,806 - INFO - Training set: 2414 samples, Test set: 604 samples\n",
      "2025-04-12 19:59:19,807 - INFO - Creating and training TF-IDF pipeline...\n",
      "2025-04-12 19:59:19,901 - INFO - TF-IDF pipeline trained successfully\n",
      "2025-04-12 19:59:19,901 - INFO - Evaluating model on test set...\n",
      "2025-04-12 19:59:19,925 - INFO - Hamming Loss: 0.0116\n",
      "2025-04-12 19:59:19,926 - INFO - Micro F1 Score: 0.9762\n",
      "2025-04-12 19:59:19,927 - INFO - Macro F1 Score: 0.9759\n",
      "2025-04-12 19:59:19,927 - INFO - Training metrics: {'hamming_loss': 0.011589403973509934, 'micro_f1': 0.9762251334303736, 'macro_f1': 0.9759012864191312}\n",
      "2025-04-12 19:59:19,928 - INFO - Saving model to hybrid_interest_classifier.pkl\n",
      "2025-04-12 19:59:19,936 - INFO - Model saved to hybrid_interest_classifier.pkl\n",
      "2025-04-12 19:59:19,937 - INFO - Testing model on example inputs...\n",
      "2025-04-12 19:59:21,177 - INFO - \n",
      "Example: 'I love hiking in the mountains and trying local foods wherever I travel.'\n",
      "2025-04-12 19:59:21,178 - INFO - Predicted interests: ['Food', 'Travel']\n",
      "2025-04-12 19:59:21,179 - INFO - Top interests by score:\n",
      "2025-04-12 19:59:21,181 - INFO -   Travel: 0.9346\n",
      "2025-04-12 19:59:21,182 - INFO -   Food: 0.8620\n",
      "2025-04-12 19:59:21,184 - INFO -   Arts: 0.1574\n",
      "2025-04-12 19:59:23,214 - INFO - \n",
      "Example: 'I'm a software developer who plays guitar in a band on weekends.'\n",
      "2025-04-12 19:59:23,217 - INFO - Predicted interests: ['Music', 'Technology']\n",
      "2025-04-12 19:59:23,220 - INFO - Top interests by score:\n",
      "2025-04-12 19:59:23,223 - INFO -   Technology: 0.7341\n",
      "2025-04-12 19:59:23,225 - INFO -   Music: 0.5906\n",
      "2025-04-12 19:59:23,229 - INFO -   Travel: 0.2487\n",
      "2025-04-12 19:59:25,938 - INFO - \n",
      "Example: 'I spend most of my time reading books and attending online courses.'\n",
      "2025-04-12 19:59:25,943 - INFO - Predicted interests: ['Education']\n",
      "2025-04-12 19:59:25,946 - INFO - Top interests by score:\n",
      "2025-04-12 19:59:25,949 - INFO -   Education: 0.7227\n",
      "2025-04-12 19:59:25,953 - INFO -   Technology: 0.3598\n",
      "2025-04-12 19:59:25,956 - INFO -   Arts: 0.2538\n",
      "2025-04-12 19:59:27,998 - INFO - \n",
      "Example: 'I enjoy painting landscapes and visiting art museums when I travel.'\n",
      "2025-04-12 19:59:28,000 - INFO - Predicted interests: ['Arts', 'Travel']\n",
      "2025-04-12 19:59:28,002 - INFO - Top interests by score:\n",
      "2025-04-12 19:59:28,006 - INFO -   Arts: 0.8923\n",
      "2025-04-12 19:59:28,009 - INFO -   Travel: 0.6424\n",
      "2025-04-12 19:59:28,010 - INFO -   Sports: 0.1331\n",
      "2025-04-12 19:59:28,013 - INFO - \n",
      "Fine-tuning alpha parameter:\n",
      "2025-04-12 19:59:30,089 - INFO - \n",
      "Alpha = 0.3 (TF-IDF weight: 0.3, BERT weight: 0.7)\n",
      "2025-04-12 19:59:30,091 - INFO - Predicted interests: ['Technology', 'Travel']\n",
      "2025-04-12 19:59:30,093 - INFO - Top 3 scores:\n",
      "2025-04-12 19:59:30,096 - INFO -   Technology: 0.9067\n",
      "2025-04-12 19:59:30,098 - INFO -   Travel: 0.5250\n",
      "2025-04-12 19:59:30,099 - INFO -   Sports: 0.1143\n",
      "2025-04-12 19:59:32,099 - INFO - \n",
      "Alpha = 0.5 (TF-IDF weight: 0.5, BERT weight: 0.5)\n",
      "2025-04-12 19:59:32,101 - INFO - Predicted interests: ['Technology']\n",
      "2025-04-12 19:59:32,103 - INFO - Top 3 scores:\n",
      "2025-04-12 19:59:32,105 - INFO -   Technology: 0.8851\n",
      "2025-04-12 19:59:32,108 - INFO -   Travel: 0.4690\n",
      "2025-04-12 19:59:32,109 - INFO -   Sports: 0.1891\n",
      "2025-04-12 19:59:34,486 - INFO - \n",
      "Alpha = 0.7 (TF-IDF weight: 0.7, BERT weight: 0.30000000000000004)\n",
      "2025-04-12 19:59:34,490 - INFO - Predicted interests: ['Technology']\n",
      "2025-04-12 19:59:34,492 - INFO - Top 3 scores:\n",
      "2025-04-12 19:59:34,494 - INFO -   Technology: 0.8635\n",
      "2025-04-12 19:59:34,497 - INFO -   Travel: 0.4131\n",
      "2025-04-12 19:59:34,499 - INFO -   Sports: 0.2639\n",
      "2025-04-12 19:59:36,845 - INFO - \n",
      "Alpha = 0.9 (TF-IDF weight: 0.9, BERT weight: 0.09999999999999998)\n",
      "2025-04-12 19:59:36,848 - INFO - Predicted interests: ['Technology']\n",
      "2025-04-12 19:59:36,850 - INFO - Top 3 scores:\n",
      "2025-04-12 19:59:36,851 - INFO -   Technology: 0.8419\n",
      "2025-04-12 19:59:36,853 - INFO -   Travel: 0.3571\n",
      "2025-04-12 19:59:36,854 - INFO -   Sports: 0.3386\n",
      "2025-04-12 19:59:36,858 - INFO - Model training and evaluation completed successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Union, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define interest categories\n",
    "INTEREST_CATEGORIES = [\"Music\", \"Food\", \"Sports\", \"Technology\", \"Arts\", \"Travel\", \"Education\"]\n",
    "\n",
    "class InterestClassifier:\n",
    "    \"\"\"\n",
    "    Hybrid Interest Classification model that combines TF-IDF with BERT zero-shot classification\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model_path: Optional[str] = None,\n",
    "                 alpha: float = 0.6, \n",
    "                 threshold: float = 0.5,\n",
    "                 bert_model_name: str = 'facebook/bart-large-mnli',\n",
    "                 use_gpu: bool = torch.cuda.is_available()):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid classifier\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to a saved model (if None, a new model will be created)\n",
    "            alpha: Weight for TF-IDF model (1-alpha for BERT)\n",
    "            threshold: Classification threshold for final predictions\n",
    "            bert_model_name: Name of the BERT model to use\n",
    "            use_gpu: Whether to use GPU for BERT inference\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.bert_model_name = bert_model_name\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        # Initialize models as None\n",
    "        self.tfidf_pipeline = None\n",
    "        self.mlb = None\n",
    "        self.bert_classifier = None\n",
    "        \n",
    "        # Load the model if path is provided\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.load_model(model_path)\n",
    "        \n",
    "        # Initialize BERT model\n",
    "        self._init_bert_classifier()\n",
    "    \n",
    "    def _improved_preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced text preprocessing that better preserves domain-specific indicators\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to preprocess\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed text\n",
    "        \"\"\"\n",
    "        # Handle potential NaN values\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters while preserving important separators\n",
    "        text = re.sub(r'[^\\w\\s|-]', ' ', text)\n",
    "        \n",
    "        # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Define domain terms dictionary\n",
    "        domain_terms = {\n",
    "            'music': ['music', 'guitar', 'band', 'concert', 'gig', 'sing', 'song', 'play music', 'musician'],\n",
    "            'food': ['food', 'cook', 'cuisine', 'recipe', 'restaurant', 'eat', 'culinary', 'bake', 'chef'],\n",
    "            'sports': ['sport', 'run', 'gym', 'fitness', 'workout', 'exercise', 'athletic', 'training'],\n",
    "            'arts': ['art', 'paint', 'draw', 'museum', 'gallery', 'exhibit', 'creative', 'design'],\n",
    "            'technology': ['tech', 'code', 'program', 'software', 'developer', 'computer', 'app', 'digital'],\n",
    "            'education': ['education', 'learn', 'course', 'class', 'study', 'book', 'read', 'academic'],\n",
    "            'travel': ['travel', 'trip', 'hike', 'explore', 'tour', 'visit', 'journey', 'destination']\n",
    "        }\n",
    "        \n",
    "        # Check for domain terms and emphasize them\n",
    "        modified_text = text\n",
    "        for category, terms in domain_terms.items():\n",
    "            for term in terms:\n",
    "                if term in text:\n",
    "                    # Add the category name explicitly if a related term is found\n",
    "                    modified_text += f\" {category} {category} {term} {term}\"\n",
    "        \n",
    "        # Split on common separators but preserve the important phrases\n",
    "        parts = []\n",
    "        for part in re.split(r'\\s*\\|\\s*', modified_text):\n",
    "            # Remove numbers (but keep words with numbers like \"web3\")\n",
    "            part = re.sub(r'\\b\\d+\\b', '', part)\n",
    "            parts.append(part)\n",
    "        \n",
    "        # Define a more focused stopwords list\n",
    "        core_stopwords = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'the', 'a', 'an', 'and', 'but', \n",
    "                          'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n",
    "                          'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', \n",
    "                          'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "                          'under', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were'}\n",
    "        \n",
    "        # Process each part and filter stopwords\n",
    "        processed_parts = []\n",
    "        for part in parts:\n",
    "            words = part.split()\n",
    "            filtered_words = [word for word in words if word not in core_stopwords]\n",
    "            \n",
    "            if filtered_words:\n",
    "                processed_parts.append(' '.join(filtered_words))\n",
    "        \n",
    "        # Join the processed parts back\n",
    "        processed_text = ' '.join(processed_parts)\n",
    "        \n",
    "        return processed_text.strip()\n",
    "    \n",
    "    def _init_bert_classifier(self):\n",
    "        \"\"\"Initialize the BERT zero-shot classifier\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Initializing BERT zero-shot classifier with model: {self.bert_model_name}\")\n",
    "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
    "            self.bert_classifier = pipeline('zero-shot-classification', \n",
    "                                           model=self.bert_model_name, \n",
    "                                           device=device)\n",
    "            logger.info(\"BERT classifier successfully initialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize BERT classifier: {e}\")\n",
    "            logger.warning(\"Proceeding without BERT - will use TF-IDF only\")\n",
    "            self.bert_classifier = None\n",
    "    \n",
    "    def train(self, \n",
    "              df: pd.DataFrame, \n",
    "              text_column: str = 'survey_answer', \n",
    "              labels_column: str = 'labels_list',\n",
    "              test_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Train the TF-IDF + Logistic Regression model\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing survey responses and labels\n",
    "            text_column: Column name containing the survey responses\n",
    "            labels_column: Column name containing the labels\n",
    "            test_size: Proportion of data to use for testing\n",
    "        \n",
    "        Returns:\n",
    "            Evaluation metrics on test set\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting model training...\")\n",
    "        \n",
    "        # Prepare labels\n",
    "        if isinstance(df[labels_column].iloc[0], str):\n",
    "            logger.info(\"Converting labels from string to list...\")\n",
    "            # Convert string representation of lists to actual lists\n",
    "            df[labels_column] = df[labels_column].str.strip('[]').str.split(',')\n",
    "            # Clean up any extra quotes or spaces\n",
    "            df[labels_column] = df[labels_column].apply(lambda x: [item.strip().strip(\"'\\\"\") for item in x])\n",
    "        \n",
    "        # Preprocess text\n",
    "        logger.info(\"Preprocessing text data...\")\n",
    "        df['processed_text'] = df[text_column].apply(self._improved_preprocess_text)\n",
    "        \n",
    "        # Initialize MultiLabelBinarizer\n",
    "        self.mlb = MultiLabelBinarizer(classes=INTEREST_CATEGORIES)\n",
    "        y = self.mlb.fit_transform(df[labels_column])\n",
    "        logger.info(f\"Target shape: {y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['processed_text'], y, test_size=test_size, random_state=42, shuffle=True\n",
    "        )\n",
    "        logger.info(f\"Training set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Create TF-IDF pipeline\n",
    "        logger.info(\"Creating and training TF-IDF pipeline...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=3000,\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            ngram_range=(1, 3),\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        lr_clf = LogisticRegression(\n",
    "            C=0.5,\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            solver='liblinear',\n",
    "            penalty='l2'\n",
    "        )\n",
    "        \n",
    "        multi_lr = MultiOutputClassifier(lr_clf)\n",
    "        \n",
    "        self.tfidf_pipeline = Pipeline([\n",
    "            ('tfidf', tfidf_vectorizer),\n",
    "            ('classifier', multi_lr)\n",
    "        ])\n",
    "        \n",
    "        # Train the pipeline\n",
    "        self.tfidf_pipeline.fit(X_train, y_train)\n",
    "        logger.info(\"TF-IDF pipeline trained successfully\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        logger.info(\"Evaluating model on test set...\")\n",
    "        y_pred = self.tfidf_pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import hamming_loss, f1_score, precision_score, recall_score\n",
    "        h_loss = hamming_loss(y_test, y_pred)\n",
    "        micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        logger.info(f\"Hamming Loss: {h_loss:.4f}\")\n",
    "        logger.info(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "        logger.info(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'hamming_loss': h_loss,\n",
    "            'micro_f1': micro_f1,\n",
    "            'macro_f1': macro_f1\n",
    "        }\n",
    "    \n",
    "    def get_tfidf_predictions(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get predictions from TF-IDF model with confidence scores\n",
    "        \n",
    "        Args:\n",
    "            text: The input text to classify\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of label -> score\n",
    "        \"\"\"\n",
    "        if self.tfidf_pipeline is None:\n",
    "            raise ValueError(\"TF-IDF model is not trained yet. Call train() first.\")\n",
    "            \n",
    "        # Preprocess text\n",
    "        processed_text = self._improved_preprocess_text(text)\n",
    "        \n",
    "        # Get raw prediction probabilities\n",
    "        y_proba = self.tfidf_pipeline.predict_proba([processed_text])\n",
    "        \n",
    "        # Convert to dictionary of label -> score\n",
    "        scores = {}\n",
    "        for i, label in enumerate(self.mlb.classes_):\n",
    "            # For MultiOutputClassifier, each element of y_proba is a list of arrays\n",
    "            # Each array is for one label and has 2 values: [prob_for_0, prob_for_1]\n",
    "            scores[label] = y_proba[i][0][1]  # Get probability of positive class\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_bert_predictions(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get predictions from BERT model\n",
    "        \n",
    "        Args:\n",
    "            text: The input text to classify\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of label -> score\n",
    "        \"\"\"\n",
    "        if self.bert_classifier is None:\n",
    "            logger.warning(\"BERT classifier is not available, returning empty scores\")\n",
    "            return {label: 0.0 for label in INTEREST_CATEGORIES}\n",
    "            \n",
    "        try:\n",
    "            # Use the BERT zero-shot classifier\n",
    "            result = self.bert_classifier(text, INTEREST_CATEGORIES, multi_label=True)\n",
    "            \n",
    "            # Convert to dictionary of label -> score\n",
    "            scores = dict(zip(result['labels'], result['scores']))\n",
    "            \n",
    "            # Ensure all categories are present (BERT may return in different order)\n",
    "            for category in INTEREST_CATEGORIES:\n",
    "                if category not in scores:\n",
    "                    scores[category] = 0.0\n",
    "                    \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in BERT prediction: {e}\")\n",
    "            return {label: 0.0 for label in INTEREST_CATEGORIES}\n",
    "    \n",
    "    def predict(self, \n",
    "                text: str, \n",
    "                alpha: Optional[float] = None,\n",
    "                threshold: Optional[float] = None,\n",
    "                return_scores: bool = False) -> Union[List[str], Dict]:\n",
    "        \"\"\"\n",
    "        Combine TF-IDF and BERT predictions using weighted average\n",
    "        \n",
    "        Args:\n",
    "            text: The input text to classify\n",
    "            alpha: Weight for TF-IDF predictions (1-alpha for BERT), uses self.alpha if None\n",
    "            threshold: Threshold for classification, uses self.threshold if None\n",
    "            return_scores: Whether to return scores along with labels\n",
    "        \n",
    "        Returns:\n",
    "            Either a list of predicted labels or a dictionary with labels and scores\n",
    "        \"\"\"\n",
    "        if self.tfidf_pipeline is None:\n",
    "            raise ValueError(\"Model is not trained yet. Call train() first.\")\n",
    "            \n",
    "        # Use instance values if not provided\n",
    "        alpha = alpha if alpha is not None else self.alpha\n",
    "        threshold = threshold if threshold is not None else self.threshold\n",
    "        \n",
    "        # Time the predictions\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get TF-IDF predictions\n",
    "        tfidf_scores = self.get_tfidf_predictions(text)\n",
    "        tfidf_time = time.time() - start_time\n",
    "        \n",
    "        # Get BERT predictions if available\n",
    "        bert_time_start = time.time()\n",
    "        if self.bert_classifier is not None:\n",
    "            bert_scores = self.get_bert_predictions(text)\n",
    "            use_bert = True\n",
    "        else:\n",
    "            bert_scores = {category: 0.0 for category in INTEREST_CATEGORIES}\n",
    "            use_bert = False\n",
    "            logger.warning(\"BERT classifier not available, using TF-IDF only\")\n",
    "        bert_time = time.time() - bert_time_start\n",
    "        \n",
    "        # Combine predictions\n",
    "        combined_scores = {}\n",
    "        final_labels = []\n",
    "        \n",
    "        for category in INTEREST_CATEGORIES:\n",
    "            # Get scores from both models\n",
    "            tfidf_score = tfidf_scores.get(category, 0.0)\n",
    "            bert_score = bert_scores.get(category, 0.0)\n",
    "            \n",
    "            # Weighted average (if using BERT)\n",
    "            if use_bert:\n",
    "                final_score = (alpha * tfidf_score) + ((1 - alpha) * bert_score)\n",
    "            else:\n",
    "                final_score = tfidf_score\n",
    "                \n",
    "            combined_scores[category] = final_score\n",
    "            \n",
    "            # Apply threshold\n",
    "            if final_score >= threshold:\n",
    "                final_labels.append(category)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        if return_scores:\n",
    "            # Sort scores for easier interpretation\n",
    "            sorted_scores = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return {\n",
    "                'labels': final_labels,\n",
    "                'scores': combined_scores,\n",
    "                'sorted_scores': sorted_scores,\n",
    "                'tfidf_scores': tfidf_scores,\n",
    "                'bert_scores': bert_scores,\n",
    "                'timing': {\n",
    "                    'tfidf': tfidf_time,\n",
    "                    'bert': bert_time,\n",
    "                    'total': total_time\n",
    "                },\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'using_bert': use_bert\n",
    "            }\n",
    "        \n",
    "        return final_labels\n",
    "    \n",
    "    def save_model(self, path: str = \"hybrid_interest_classifier.pkl\"):\n",
    "        \"\"\"\n",
    "        Save the model to disk\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save the model\n",
    "        \"\"\"\n",
    "        if self.tfidf_pipeline is None:\n",
    "            raise ValueError(\"Model is not trained yet. Call train() first.\")\n",
    "            \n",
    "        # Note: We only save the TF-IDF pipeline and MLBinarizer\n",
    "        # BERT will be re-initialized on load\n",
    "        components = {\n",
    "            'tfidf_pipeline': self.tfidf_pipeline,\n",
    "            'mlb': self.mlb,\n",
    "            'alpha': self.alpha,\n",
    "            'threshold': self.threshold,\n",
    "            'bert_model_name': self.bert_model_name,\n",
    "            'interest_categories': INTEREST_CATEGORIES,\n",
    "            'version': '1.0'\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(components, f)\n",
    "            \n",
    "        logger.info(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"\n",
    "        Load a saved model from disk\n",
    "        \n",
    "        Args:\n",
    "            path: Path to the saved model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                components = pickle.load(f)\n",
    "                \n",
    "            self.tfidf_pipeline = components['tfidf_pipeline']\n",
    "            self.mlb = components['mlb']\n",
    "            self.alpha = components.get('alpha', 0.6)\n",
    "            self.threshold = components.get('threshold', 0.5)\n",
    "            self.bert_model_name = components.get('bert_model_name', 'facebook/bart-large-mnli')\n",
    "            \n",
    "            logger.info(f\"Model loaded from {path}\")\n",
    "            \n",
    "            # Re-initialize BERT classifier\n",
    "            self._init_bert_classifier()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    try:\n",
    "        # Load dataset\n",
    "        logger.info(\"Loading dataset: survey_interest_dataset_enhanced.csv\")\n",
    "        df = pd.read_csv('survey_interest_dataset_enhanced.csv')\n",
    "        \n",
    "        # Convert labels_list if it's a string representation\n",
    "        if 'labels_list' in df.columns and isinstance(df['labels_list'].iloc[0], str):\n",
    "            logger.info(\"Converting labels_list from string to list...\")\n",
    "            df['labels_list'] = df['labels_list'].str.strip('[]').str.split(',')\n",
    "            df['labels_list'] = df['labels_list'].apply(lambda x: [item.strip().strip(\"'\\\"\") for item in x])\n",
    "        \n",
    "        # Initialize classifier\n",
    "        logger.info(\"Initializing classifier with alpha=0.6, threshold=0.5\")\n",
    "        classifier = InterestClassifier(alpha=0.6, threshold=0.5)\n",
    "        \n",
    "        # Train the model\n",
    "        logger.info(\"Training the model...\")\n",
    "        metrics = classifier.train(df)\n",
    "        logger.info(f\"Training metrics: {metrics}\")\n",
    "        \n",
    "        # Save the model\n",
    "        model_path = \"hybrid_interest_classifier.pkl\"\n",
    "        logger.info(f\"Saving model to {model_path}\")\n",
    "        classifier.save_model(model_path)\n",
    "        \n",
    "        # Test on some examples\n",
    "        test_examples = [\n",
    "            \"I love hiking in the mountains and trying local foods wherever I travel.\",\n",
    "            \"I'm a software developer who plays guitar in a band on weekends.\",\n",
    "            \"I spend most of my time reading books and attending online courses.\",\n",
    "            \"I enjoy painting landscapes and visiting art museums when I travel.\"\n",
    "        ]\n",
    "        \n",
    "        logger.info(\"Testing model on example inputs...\")\n",
    "        for example in test_examples:\n",
    "            result = classifier.predict(example, return_scores=True)\n",
    "            logger.info(f\"\\nExample: '{example}'\")\n",
    "            logger.info(f\"Predicted interests: {result['labels']}\")\n",
    "            logger.info(\"Top interests by score:\")\n",
    "            for category, score in result['sorted_scores'][:3]:\n",
    "                logger.info(f\"  {category}: {score:.4f}\")\n",
    "                \n",
    "        # Fine-tuning alpha parameter demo\n",
    "        logger.info(\"\\nFine-tuning alpha parameter:\")\n",
    "        example = \"I work as a software developer and enjoy hiking on weekends\"\n",
    "        for alpha in [0.3, 0.5, 0.7, 0.9]:\n",
    "            result = classifier.predict(example, alpha=alpha, return_scores=True)\n",
    "            logger.info(f\"\\nAlpha = {alpha} (TF-IDF weight: {alpha}, BERT weight: {1-alpha})\")\n",
    "            logger.info(f\"Predicted interests: {result['labels']}\")\n",
    "            logger.info(\"Top 3 scores:\")\n",
    "            for category, score in result['sorted_scores'][:3]:\n",
    "                logger.info(f\"  {category}: {score:.4f}\")\n",
    "        \n",
    "        logger.info(\"Model training and evaluation completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main function: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02eefd8",
   "metadata": {},
   "source": [
    "### Hybrid Model Fine-Tuning Script Code\n",
    "#### Optimized Model Tuning Code with Checkpoint Support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169573bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 14:11:36,982 - INFO - Dataset loaded with 3018 rows\n",
      "2025-04-12 14:11:36,984 - INFO - Step 1: Tuning alpha parameter\n",
      "2025-04-12 14:11:36,985 - INFO - Starting alpha parameter tuning...\n",
      "2025-04-12 14:11:36,996 - INFO - Processing fold 1/3\n",
      "2025-04-12 14:11:36,999 - INFO - Initializing BERT zero-shot classifier with model: facebook/bart-large-mnli\n",
      "Device set to use cpu\n",
      "2025-04-12 14:11:37,714 - INFO - BERT classifier successfully initialized\n",
      "2025-04-12 14:11:37,715 - INFO - Starting model training...\n",
      "2025-04-12 14:11:37,716 - INFO - Preprocessing text data...\n",
      "2025-04-12 14:11:37,760 - INFO - Target shape: (2012, 7)\n",
      "2025-04-12 14:11:37,762 - INFO - Training set: 1609 samples, Test set: 403 samples\n",
      "2025-04-12 14:11:37,763 - INFO - Creating and training TF-IDF pipeline...\n",
      "2025-04-12 14:11:37,815 - INFO - TF-IDF pipeline trained successfully\n",
      "2025-04-12 14:11:37,815 - INFO - Evaluating model on test set...\n",
      "2025-04-12 14:11:37,831 - INFO - Hamming Loss: 0.0181\n",
      "2025-04-12 14:11:37,831 - INFO - Micro F1 Score: 0.9610\n",
      "2025-04-12 14:11:37,832 - INFO - Macro F1 Score: 0.9584\n",
      "2025-04-12 14:11:37,832 - INFO - Testing alpha=0.3, threshold=0.3\n",
      "Fold 1, Alpha=0.3, Threshold=0.3: 100%|██████████| 1006/1006 [25:14<00:00,  1.51s/it]  \n",
      "2025-04-12 14:36:51,917 - INFO - Fold 1, Alpha=0.3, Threshold=0.3: Micro-F1=0.7998, Macro-F1=0.8067, Hamming Loss=0.1000\n",
      "2025-04-12 14:36:51,921 - INFO - Testing alpha=0.3, threshold=0.4\n",
      "Fold 1, Alpha=0.3, Threshold=0.4:  24%|██▎       | 237/1006 [04:19<14:02,  1.10s/it]  \n",
      "2025-04-12 14:41:11,517 - WARNING - KeyboardInterrupt detected. Saving checkpoint and exiting...\n",
      "2025-04-12 14:41:11,519 - WARNING - KeyboardInterrupt received. Saving progress and attempting to continue with analysis...\n",
      "2025-04-12 14:41:11,528 - INFO - Best parameters by Micro-F1: alpha=0.3,threshold=0.3, score: 0.7998\n",
      "2025-04-12 14:41:11,529 - INFO - Best parameters by Hamming Loss: alpha=0.3,threshold=0.3, score: 0.1000\n",
      "2025-04-12 14:41:12,827 - INFO - Using parameters: alpha=0.3, threshold=0.3\n",
      "2025-04-12 14:41:12,829 - INFO - Step 2: Comparing hybrid vs TF-IDF only vs BERT only\n",
      "2025-04-12 14:41:12,830 - INFO - Evaluating hybrid model vs TF-IDF only...\n",
      "2025-04-12 14:41:12,837 - INFO - Training hybrid model...\n",
      "2025-04-12 14:41:12,887 - INFO - Initializing BERT zero-shot classifier with model: facebook/bart-large-mnli\n",
      "Device set to use cpu\n",
      "2025-04-12 14:41:14,104 - INFO - BERT classifier successfully initialized\n",
      "2025-04-12 14:41:14,104 - INFO - Starting model training...\n",
      "2025-04-12 14:41:14,105 - INFO - Preprocessing text data...\n",
      "2025-04-12 14:41:14,157 - INFO - Target shape: (2414, 7)\n",
      "2025-04-12 14:41:14,160 - INFO - Training set: 1931 samples, Test set: 483 samples\n",
      "2025-04-12 14:41:14,160 - INFO - Creating and training TF-IDF pipeline...\n",
      "2025-04-12 14:41:14,239 - INFO - TF-IDF pipeline trained successfully\n",
      "2025-04-12 14:41:14,240 - INFO - Evaluating model on test set...\n",
      "2025-04-12 14:41:14,266 - INFO - Hamming Loss: 0.0157\n",
      "2025-04-12 14:41:14,267 - INFO - Micro F1 Score: 0.9675\n",
      "2025-04-12 14:41:14,269 - INFO - Macro F1 Score: 0.9664\n",
      "2025-04-12 14:41:14,270 - INFO - Training TF-IDF only model...\n",
      "2025-04-12 14:41:14,271 - INFO - Initializing BERT zero-shot classifier with model: facebook/bart-large-mnli\n",
      "Device set to use cpu\n",
      "2025-04-12 14:41:14,860 - INFO - BERT classifier successfully initialized\n",
      "2025-04-12 14:41:14,861 - INFO - Starting model training...\n",
      "2025-04-12 14:41:14,862 - INFO - Preprocessing text data...\n",
      "2025-04-12 14:41:14,904 - INFO - Target shape: (2414, 7)\n",
      "2025-04-12 14:41:14,906 - INFO - Training set: 1931 samples, Test set: 483 samples\n",
      "2025-04-12 14:41:14,907 - INFO - Creating and training TF-IDF pipeline...\n",
      "2025-04-12 14:41:14,970 - INFO - TF-IDF pipeline trained successfully\n",
      "2025-04-12 14:41:14,971 - INFO - Evaluating model on test set...\n",
      "2025-04-12 14:41:14,984 - INFO - Hamming Loss: 0.0157\n",
      "2025-04-12 14:41:14,985 - INFO - Micro F1 Score: 0.9675\n",
      "2025-04-12 14:41:14,986 - INFO - Macro F1 Score: 0.9664\n",
      "2025-04-12 14:41:14,986 - INFO - Training BERT-only model...\n",
      "2025-04-12 14:41:14,987 - INFO - Initializing BERT zero-shot classifier with model: facebook/bart-large-mnli\n",
      "Device set to use cpu\n",
      "2025-04-12 14:41:15,731 - INFO - BERT classifier successfully initialized\n",
      "2025-04-12 14:41:15,732 - INFO - Starting model training...\n",
      "2025-04-12 14:41:15,733 - INFO - Preprocessing text data...\n",
      "2025-04-12 14:41:15,778 - INFO - Target shape: (2414, 7)\n",
      "2025-04-12 14:41:15,781 - INFO - Training set: 1931 samples, Test set: 483 samples\n",
      "2025-04-12 14:41:15,782 - INFO - Creating and training TF-IDF pipeline...\n",
      "2025-04-12 14:41:15,861 - INFO - TF-IDF pipeline trained successfully\n",
      "2025-04-12 14:41:15,862 - INFO - Evaluating model on test set...\n",
      "2025-04-12 14:41:15,878 - INFO - Hamming Loss: 0.0157\n",
      "2025-04-12 14:41:15,879 - INFO - Micro F1 Score: 0.9675\n",
      "2025-04-12 14:41:15,879 - INFO - Macro F1 Score: 0.9664\n",
      "2025-04-12 14:41:15,880 - INFO - Evaluating Hybrid (BERT + TF-IDF)...\n",
      "Testing Hybrid (BERT + TF-IDF): 100%|██████████| 604/604 [3:11:59<00:00, 19.07s/it]     \n",
      "2025-04-12 17:53:14,916 - INFO - Hybrid (BERT + TF-IDF) results:\n",
      "2025-04-12 17:53:14,916 - INFO -   Micro-F1: 0.7971\n",
      "2025-04-12 17:53:14,917 - INFO -   Macro-F1: 0.8036\n",
      "2025-04-12 17:53:14,918 - INFO -   Hamming Loss: 0.1008\n",
      "2025-04-12 17:53:14,918 - INFO -   Avg. Prediction Time: 19.0690 seconds\n",
      "2025-04-12 17:53:14,919 - INFO -   Samples evaluated: 604 of 604\n",
      "2025-04-12 17:53:14,919 - INFO - Evaluating TF-IDF Only...\n",
      "Testing TF-IDF Only:  15%|█▌        | 93/604 [01:22<06:38,  1.28it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "# Import the hybrid classifier\n",
    "from hybrid_interest_classifier import InterestClassifier\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def tune_alpha_parameter(df, alphas=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "                         n_splits=5, thresholds=[0.3, 0.4, 0.5, 0.6],\n",
    "                         checkpoint_file='alpha_tuning_checkpoint.pkl',\n",
    "                         resume=True):\n",
    "    \"\"\"\n",
    "    Fine-tune the alpha parameter using cross-validation with checkpoint support\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the dataset\n",
    "        alphas: List of alpha values to test\n",
    "        n_splits: Number of CV splits\n",
    "        thresholds: List of thresholds to test\n",
    "        checkpoint_file: File to save/load checkpoints\n",
    "        resume: Whether to resume from checkpoint if available\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of results\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting alpha parameter tuning...\")\n",
    "    \n",
    "    # Convert labels_list if it's a string representation\n",
    "    if isinstance(df['labels_list'].iloc[0], str):\n",
    "        df['labels_list'] = df['labels_list'].str.strip('[]').str.split(',')\n",
    "        df['labels_list'] = df['labels_list'].apply(lambda x: [item.strip().strip(\"'\\\"\") for item in x])\n",
    "    \n",
    "    # Initialize CrossValidator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Check if checkpoint exists and we want to resume\n",
    "    if os.path.exists(checkpoint_file) and resume:\n",
    "        try:\n",
    "            logger.info(f\"Loading checkpoint from {checkpoint_file}\")\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                checkpoint = pickle.load(f)\n",
    "            \n",
    "            results = checkpoint['results']\n",
    "            start_fold = checkpoint['next_fold']\n",
    "            start_alpha_idx = checkpoint['next_alpha_idx']\n",
    "            start_threshold_idx = checkpoint['next_threshold_idx']\n",
    "            \n",
    "            logger.info(f\"Resuming from fold {start_fold}, alpha_idx {start_alpha_idx}, threshold_idx {start_threshold_idx}\")\n",
    "            \n",
    "            # Get all train/test splits to skip to the right fold\n",
    "            all_splits = list(kf.split(df))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading checkpoint: {e}\")\n",
    "            logger.info(\"Starting from scratch...\")\n",
    "            results = {\n",
    "                'params': [],\n",
    "                'micro_f1': [],\n",
    "                'macro_f1': [],\n",
    "                'hamming': []\n",
    "            }\n",
    "            start_fold = 0\n",
    "            start_alpha_idx = 0\n",
    "            start_threshold_idx = 0\n",
    "            all_splits = list(kf.split(df))\n",
    "    else:\n",
    "        # Initialize results\n",
    "        results = {\n",
    "            'params': [],\n",
    "            'micro_f1': [],\n",
    "            'macro_f1': [],\n",
    "            'hamming': []\n",
    "        }\n",
    "        start_fold = 0\n",
    "        start_alpha_idx = 0\n",
    "        start_threshold_idx = 0\n",
    "        all_splits = list(kf.split(df))\n",
    "    \n",
    "    # Run cross-validation with resume capability\n",
    "    try:\n",
    "        # Loop through folds\n",
    "        for fold_idx in range(start_fold, n_splits):\n",
    "            logger.info(f\"Processing fold {fold_idx+1}/{n_splits}\")\n",
    "            \n",
    "            # Get split\n",
    "            train_idx, test_idx = all_splits[fold_idx]\n",
    "            \n",
    "            # Split data\n",
    "            train_df = df.iloc[train_idx].copy()\n",
    "            test_df = df.iloc[test_idx].copy()\n",
    "            \n",
    "            # Initialize and train classifier\n",
    "            classifier = InterestClassifier(alpha=0.5, threshold=0.5)  # Default values\n",
    "            classifier.train(train_df)\n",
    "            \n",
    "            # Test each combination of alpha and threshold\n",
    "            for alpha_idx, alpha in enumerate(alphas[start_alpha_idx if fold_idx == start_fold else 0:]):\n",
    "                current_alpha_idx = start_alpha_idx + alpha_idx if fold_idx == start_fold else alpha_idx\n",
    "                \n",
    "                for threshold_idx, threshold in enumerate(thresholds[start_threshold_idx if fold_idx == start_fold and alpha_idx == 0 else 0:]):\n",
    "                    current_threshold_idx = start_threshold_idx + threshold_idx if fold_idx == start_fold and alpha_idx == 0 else threshold_idx\n",
    "                    \n",
    "                    logger.info(f\"Testing alpha={alpha}, threshold={threshold}\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Get predictions for test set\n",
    "                        y_true = []\n",
    "                        y_pred = []\n",
    "                        \n",
    "                        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), \n",
    "                                            desc=f\"Fold {fold_idx+1}, Alpha={alpha}, Threshold={threshold}\"):\n",
    "                            text = row['survey_answer']\n",
    "                            true_labels = row['labels_list']\n",
    "                            \n",
    "                            # Get predictions with current parameters\n",
    "                            # Add timeout or other safeguards if prediction takes too long\n",
    "                            try:\n",
    "                                pred_labels = classifier.predict(text, alpha=alpha, threshold=threshold)\n",
    "                                \n",
    "                                y_true.append(true_labels)\n",
    "                                y_pred.append(pred_labels)\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"Error predicting sample {idx}: {e}\")\n",
    "                                # Use an empty prediction in case of error\n",
    "                                y_true.append(true_labels)\n",
    "                                y_pred.append([])\n",
    "                        \n",
    "                        # Convert to multilabel format\n",
    "                        from sklearn.preprocessing import MultiLabelBinarizer\n",
    "                        mlb = MultiLabelBinarizer(classes=classifier.mlb.classes_)\n",
    "                        y_true_bin = mlb.fit_transform(y_true)\n",
    "                        y_pred_bin = mlb.transform(y_pred)\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        micro_f1 = f1_score(y_true_bin, y_pred_bin, average='micro')\n",
    "                        macro_f1 = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
    "                        h_loss = hamming_loss(y_true_bin, y_pred_bin)\n",
    "                        \n",
    "                        # Store results\n",
    "                        results['params'].append((fold_idx+1, alpha, threshold))\n",
    "                        results['micro_f1'].append(micro_f1)\n",
    "                        results['macro_f1'].append(macro_f1)\n",
    "                        results['hamming'].append(h_loss)\n",
    "                        \n",
    "                        logger.info(f\"Fold {fold_idx+1}, Alpha={alpha}, Threshold={threshold}: \"\n",
    "                                  f\"Micro-F1={micro_f1:.4f}, Macro-F1={macro_f1:.4f}, \"\n",
    "                                  f\"Hamming Loss={h_loss:.4f}\")\n",
    "                        \n",
    "                        # Save checkpoint after each parameter evaluation\n",
    "                        checkpoint = {\n",
    "                            'results': results,\n",
    "                            'next_fold': fold_idx,\n",
    "                            'next_alpha_idx': current_alpha_idx,\n",
    "                            'next_threshold_idx': current_threshold_idx + 1 if current_threshold_idx + 1 < len(thresholds) else 0,\n",
    "                            'next_param_set': (\n",
    "                                fold_idx,\n",
    "                                current_alpha_idx + 1 if current_threshold_idx + 1 >= len(thresholds) else current_alpha_idx,\n",
    "                                0 if current_threshold_idx + 1 >= len(thresholds) else current_threshold_idx + 1\n",
    "                            )\n",
    "                        }\n",
    "                        \n",
    "                        # If we've completed this alpha, advance to next alpha\n",
    "                        if current_threshold_idx + 1 >= len(thresholds):\n",
    "                            checkpoint['next_alpha_idx'] = current_alpha_idx + 1\n",
    "                            checkpoint['next_threshold_idx'] = 0\n",
    "                        \n",
    "                        # If we've completed all alphas, advance to next fold\n",
    "                        if current_alpha_idx + 1 >= len(alphas) and current_threshold_idx + 1 >= len(thresholds):\n",
    "                            checkpoint['next_fold'] = fold_idx + 1\n",
    "                            checkpoint['next_alpha_idx'] = 0\n",
    "                            checkpoint['next_threshold_idx'] = 0\n",
    "                        \n",
    "                        with open(checkpoint_file, 'wb') as f:\n",
    "                            pickle.dump(checkpoint, f)\n",
    "                        \n",
    "                    except KeyboardInterrupt:\n",
    "                        logger.warning(\"KeyboardInterrupt detected. Saving checkpoint and exiting...\")\n",
    "                        # Save checkpoint before exiting\n",
    "                        checkpoint = {\n",
    "                            'results': results,\n",
    "                            'next_fold': fold_idx,\n",
    "                            'next_alpha_idx': current_alpha_idx,\n",
    "                            'next_threshold_idx': current_threshold_idx,\n",
    "                            'next_param_set': (fold_idx, current_alpha_idx, current_threshold_idx)\n",
    "                        }\n",
    "                        with open(checkpoint_file, 'wb') as f:\n",
    "                            pickle.dump(checkpoint, f)\n",
    "                        raise\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error evaluating params alpha={alpha}, threshold={threshold}: {e}\")\n",
    "                        logger.error(traceback.format_exc())\n",
    "                        continue\n",
    "                \n",
    "                # Reset threshold index for next alpha\n",
    "                start_threshold_idx = 0\n",
    "            \n",
    "            # Reset alpha index for next fold\n",
    "            start_alpha_idx = 0\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"KeyboardInterrupt received. Saving progress and attempting to continue with analysis...\")\n",
    "        # We'll still try to produce meaningful results with what we have so far\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during tuning: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    # Analyze results even if we didn't complete all runs\n",
    "    if len(results['params']) == 0:\n",
    "        logger.error(\"No results collected. Cannot analyze.\")\n",
    "        return None\n",
    "    \n",
    "    # Aggregate results by parameters\n",
    "    agg_results = {}\n",
    "    for param in [(a, t) for a in alphas for t in thresholds]:\n",
    "        alpha, threshold = param\n",
    "        mask = [(fold, a, t) for fold, a, t in results['params'] if a == alpha and t == threshold]\n",
    "        \n",
    "        if not mask:  # Skip if we have no results for this parameter combination\n",
    "            continue\n",
    "        \n",
    "        # Calculate mean and std for each metric\n",
    "        micro_f1_values = [results['micro_f1'][results['params'].index(key)] for key in mask]\n",
    "        macro_f1_values = [results['macro_f1'][results['params'].index(key)] for key in mask]\n",
    "        hamming_values = [results['hamming'][results['params'].index(key)] for key in mask]\n",
    "        \n",
    "        agg_results[f\"alpha={alpha},threshold={threshold}\"] = {\n",
    "            'micro_f1': {\n",
    "                'mean': np.mean(micro_f1_values),\n",
    "                'std': np.std(micro_f1_values)\n",
    "            },\n",
    "            'macro_f1': {\n",
    "                'mean': np.mean(macro_f1_values),\n",
    "                'std': np.std(macro_f1_values)\n",
    "            },\n",
    "            'hamming': {\n",
    "                'mean': np.mean(hamming_values),\n",
    "                'std': np.std(hamming_values)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Find best parameters\n",
    "    best_micro_f1 = 0\n",
    "    best_params_micro = None\n",
    "    best_hamming = 1.0  # Lower is better for hamming loss\n",
    "    best_params_hamming = None\n",
    "    \n",
    "    for param, metrics in agg_results.items():\n",
    "        if metrics['micro_f1']['mean'] > best_micro_f1:\n",
    "            best_micro_f1 = metrics['micro_f1']['mean']\n",
    "            best_params_micro = param\n",
    "        \n",
    "        if metrics['hamming']['mean'] < best_hamming:\n",
    "            best_hamming = metrics['hamming']['mean']\n",
    "            best_params_hamming = param\n",
    "    \n",
    "    if best_params_micro:\n",
    "        logger.info(f\"Best parameters by Micro-F1: {best_params_micro}, score: {best_micro_f1:.4f}\")\n",
    "    if best_params_hamming:\n",
    "        logger.info(f\"Best parameters by Hamming Loss: {best_params_hamming}, score: {best_hamming:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    with open('alpha_tuning_results.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'aggregated': agg_results,\n",
    "            'best_micro_f1': {\n",
    "                'params': best_params_micro,\n",
    "                'score': float(best_micro_f1) if best_params_micro else None\n",
    "            },\n",
    "            'best_hamming': {\n",
    "                'params': best_params_hamming,\n",
    "                'score': float(best_hamming) if best_params_hamming else None\n",
    "            },\n",
    "            'completion_status': {\n",
    "                'completed_combinations': len(results['params']),\n",
    "                'total_combinations': len(alphas) * len(thresholds) * n_splits,\n",
    "                'percentage_complete': 100 * len(results['params']) / (len(alphas) * len(thresholds) * n_splits)\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Plot results if we have enough data\n",
    "    if agg_results:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot Micro-F1 by alpha for each threshold\n",
    "        plt.subplot(2, 1, 1)\n",
    "        for threshold in thresholds:\n",
    "            x = []\n",
    "            y = []\n",
    "            yerr = []\n",
    "            for alpha in alphas:\n",
    "                key = f\"alpha={alpha},threshold={threshold}\"\n",
    "                if key in agg_results:\n",
    "                    x.append(alpha)\n",
    "                    y.append(agg_results[key]['micro_f1']['mean'])\n",
    "                    yerr.append(agg_results[key]['micro_f1']['std'])\n",
    "            \n",
    "            if x:  # Only plot if we have data\n",
    "                plt.errorbar(x, y, yerr=yerr, marker='o', label=f'Threshold={threshold}')\n",
    "        \n",
    "        plt.xlabel('Alpha (TF-IDF Weight)')\n",
    "        plt.ylabel('Micro F1-Score')\n",
    "        plt.title('Micro F1-Score by Alpha and Threshold')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot Hamming Loss by alpha for each threshold\n",
    "        plt.subplot(2, 1, 2)\n",
    "        for threshold in thresholds:\n",
    "            x = []\n",
    "            y = []\n",
    "            yerr = []\n",
    "            for alpha in alphas:\n",
    "                key = f\"alpha={alpha},threshold={threshold}\"\n",
    "                if key in agg_results:\n",
    "                    x.append(alpha)\n",
    "                    y.append(agg_results[key]['hamming']['mean'])\n",
    "                    yerr.append(agg_results[key]['hamming']['std'])\n",
    "            \n",
    "            if x:  # Only plot if we have data\n",
    "                plt.errorbar(x, y, yerr=yerr, marker='o', label=f'Threshold={threshold}')\n",
    "        \n",
    "        plt.xlabel('Alpha (TF-IDF Weight)')\n",
    "        plt.ylabel('Hamming Loss')\n",
    "        plt.title('Hamming Loss by Alpha and Threshold')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('alpha_tuning_results.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Return best parameters or defaults if not enough data\n",
    "    result = {\n",
    "        'aggregated': agg_results,\n",
    "        'best_micro_f1': {\n",
    "            'params': best_params_micro,\n",
    "            'score': best_micro_f1\n",
    "        },\n",
    "        'best_hamming': {\n",
    "            'params': best_params_hamming,\n",
    "            'score': best_hamming\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # If we don't have enough results, provide sensible defaults\n",
    "    if not best_params_micro:\n",
    "        logger.warning(\"Not enough data to determine best parameters. Using default alpha=0.6, threshold=0.5\")\n",
    "        result['best_micro_f1'] = {\n",
    "            'params': \"alpha=0.6,threshold=0.5\",\n",
    "            'score': 0.0\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def evaluate_hybrid_vs_tfidf(df, best_alpha=0.6, best_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compare hybrid model performance against TF-IDF only\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the dataset\n",
    "        best_alpha: Best alpha value from tuning\n",
    "        best_threshold: Best threshold value from tuning\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating hybrid model vs TF-IDF only...\")\n",
    "    \n",
    "    # Convert labels_list if needed\n",
    "    if isinstance(df['labels_list'].iloc[0], str):\n",
    "        df['labels_list'] = df['labels_list'].str.strip('[]').str.split(',')\n",
    "        df['labels_list'] = df['labels_list'].apply(lambda x: [item.strip().strip(\"'\\\"\") for item in x])\n",
    "    \n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train both classifiers\n",
    "    logger.info(\"Training hybrid model...\")\n",
    "    hybrid_classifier = InterestClassifier(alpha=best_alpha, threshold=best_threshold)\n",
    "    hybrid_classifier.train(train_df)\n",
    "    \n",
    "    logger.info(\"Training TF-IDF only model...\")\n",
    "    tfidf_only_classifier = InterestClassifier(alpha=1.0, threshold=best_threshold)  # Alpha=1.0 means 100% TF-IDF\n",
    "    tfidf_only_classifier.train(train_df)\n",
    "    \n",
    "    # Initialize BERT-only classifier for comparison (alpha=0 means 100% BERT)\n",
    "    logger.info(\"Training BERT-only model...\")\n",
    "    bert_only_classifier = InterestClassifier(alpha=0.0, threshold=best_threshold)\n",
    "    bert_only_classifier.train(train_df)  # We still need to train for the MLBinarizer\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    models = {\n",
    "        'Hybrid (BERT + TF-IDF)': hybrid_classifier,\n",
    "        'TF-IDF Only': tfidf_only_classifier,\n",
    "        'BERT Only': bert_only_classifier\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for model_name, classifier in models.items():\n",
    "        logger.info(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        prediction_times = []\n",
    "        \n",
    "        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"Testing {model_name}\"):\n",
    "            try:\n",
    "                text = row['survey_answer']\n",
    "                true_labels = row['labels_list']\n",
    "                \n",
    "                # Measure prediction time\n",
    "                start_time = time.time()\n",
    "                prediction = classifier.predict(text, return_scores=True)\n",
    "                pred_labels = prediction['labels']\n",
    "                end_time = time.time()\n",
    "                \n",
    "                prediction_times.append(end_time - start_time)\n",
    "                \n",
    "                y_true.append(true_labels)\n",
    "                y_pred.append(pred_labels)\n",
    "            except KeyboardInterrupt:\n",
    "                logger.warning(f\"KeyboardInterrupt during {model_name} evaluation. Processing collected data...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error predicting with {model_name} on sample {idx}: {e}\")\n",
    "                # Use empty prediction in case of error\n",
    "                y_true.append(true_labels)\n",
    "                y_pred.append([])\n",
    "        \n",
    "        # Skip further processing if we have no predictions (e.g., early interrupt)\n",
    "        if not y_pred:\n",
    "            logger.warning(f\"No predictions collected for {model_name}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Convert to multilabel format\n",
    "            from sklearn.preprocessing import MultiLabelBinarizer\n",
    "            mlb = MultiLabelBinarizer(classes=classifier.mlb.classes_)\n",
    "            y_true_bin = mlb.fit_transform(y_true)\n",
    "            y_pred_bin = mlb.transform(y_pred)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            micro_f1 = f1_score(y_true_bin, y_pred_bin, average='micro')\n",
    "            macro_f1 = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
    "            h_loss = hamming_loss(y_true_bin, y_pred_bin)\n",
    "            \n",
    "            # Average prediction time\n",
    "            avg_time = np.mean(prediction_times) if prediction_times else np.nan\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'micro_f1': micro_f1,\n",
    "                'macro_f1': macro_f1,\n",
    "                'hamming_loss': h_loss,\n",
    "                'avg_prediction_time': avg_time,\n",
    "                'samples_evaluated': len(y_pred)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"{model_name} results:\")\n",
    "            logger.info(f\"  Micro-F1: {micro_f1:.4f}\")\n",
    "            logger.info(f\"  Macro-F1: {macro_f1:.4f}\")\n",
    "            logger.info(f\"  Hamming Loss: {h_loss:.4f}\")\n",
    "            logger.info(f\"  Avg. Prediction Time: {avg_time:.4f} seconds\")\n",
    "            logger.info(f\"  Samples evaluated: {len(y_pred)} of {len(test_df)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating metrics for {model_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "    \n",
    "    # Save results\n",
    "    with open('model_comparison_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Plot comparison if we have results\n",
    "    if results:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot F1 scores\n",
    "        plt.subplot(2, 2, 1)\n",
    "        model_names = list(results.keys())\n",
    "        micro_f1_scores = [results[model]['micro_f1'] for model in model_names]\n",
    "        plt.bar(model_names, micro_f1_scores)\n",
    "        plt.ylabel('Micro F1-Score')\n",
    "        plt.title('Micro F1-Score Comparison')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Plot Macro F1\n",
    "        plt.subplot(2, 2, 2)\n",
    "        macro_f1_scores = [results[model]['macro_f1'] for model in model_names]\n",
    "        plt.bar(model_names, macro_f1_scores)\n",
    "        plt.ylabel('Macro F1-Score')\n",
    "        plt.title('Macro F1-Score Comparison')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Plot Hamming loss\n",
    "        plt.subplot(2, 2, 3)\n",
    "        hamming_scores = [results[model]['hamming_loss'] for model in model_names]\n",
    "        plt.bar(model_names, hamming_scores)\n",
    "        plt.ylabel('Hamming Loss')\n",
    "        plt.title('Hamming Loss Comparison (Lower is Better)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Plot prediction time\n",
    "        plt.subplot(2, 2, 4)\n",
    "        times = [results[model]['avg_prediction_time'] for model in model_names]\n",
    "        plt.bar(model_names, times)\n",
    "        plt.ylabel('Average Prediction Time (seconds)')\n",
    "        plt.title('Prediction Time Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison_results.png')\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the tuning process with checkpoint support\"\"\"\n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = pd.read_csv('survey_interest_dataset_enhanced.csv')\n",
    "        logger.info(f\"Dataset loaded with {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Tune alpha parameter with checkpoint support\n",
    "        logger.info(\"Step 1: Tuning alpha parameter\")\n",
    "        tuning_results = tune_alpha_parameter(\n",
    "            df,\n",
    "            alphas=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "            thresholds=[0.3, 0.4, 0.5, 0.6],\n",
    "            n_splits=3,  # Using 3 folds for speed\n",
    "            resume=True  # Enable checkpoint resuming\n",
    "        )\n",
    "        \n",
    "        # If tuning was interrupted or failed, use default values\n",
    "        if tuning_results is None or 'best_micro_f1' not in tuning_results or tuning_results['best_micro_f1']['params'] is None:\n",
    "            logger.warning(\"Tuning did not complete successfully. Using default parameters.\")\n",
    "            best_alpha = 0.6  # Default value\n",
    "            best_threshold = 0.5  # Default value\n",
    "        else:\n",
    "            # Extract best parameters\n",
    "            best_param_str = tuning_results['best_micro_f1']['params']\n",
    "            best_alpha = float(best_param_str.split(',')[0].split('=')[1])\n",
    "            best_threshold = float(best_param_str.split(',')[1].split('=')[1])\n",
    "        \n",
    "        logger.info(f\"Using parameters: alpha={best_alpha}, threshold={best_threshold}\")\n",
    "        \n",
    "        # Step 2: Compare models\n",
    "        logger.info(\"Step 2: Comparing hybrid vs TF-IDF only vs BERT only\")\n",
    "        comparison_results = evaluate_hybrid_vs_tfidf(df, best_alpha, best_threshold)\n",
    "        \n",
    "        # Step 3: Error analysis - only if we have comparison results\n",
    "        if comparison_results and 'Hybrid (BERT + TF-IDF)' in comparison_results:\n",
    "            logger.info(\"Step 3: Analyzing error cases\")\n",
    "            classifier = InterestClassifier(alpha=best_alpha, threshold=best_threshold)\n",
    "            classifier.train(df)\n",
    "            error_analysis = analyze_error_cases(df, classifier, n_examples=20)\n",
    "        else:\n",
    "            logger.warning(\"Skipping error analysis due to incomplete comparison results\")\n",
    "        \n",
    "        logger.info(\"Model tuning and analysis complete!\")\n",
    "        logger.info(f\"Best alpha: {best_alpha}\")\n",
    "        logger.info(f\"Best threshold: {best_threshold}\")\n",
    "        logger.info(\"Results are saved to:\")\n",
    "        logger.info(\"  - alpha_tuning_results.json\")\n",
    "        logger.info(\"  - alpha_tuning_results.png\")\n",
    "        logger.info(\"  - model_comparison_results.json\")\n",
    "        logger.info(\"  - model_comparison_results.png\")\n",
    "        logger.info(\"  - error_analysis.txt\")\n",
    "        logger.info(\"  - error_analysis.json\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"\\nProcess was interrupted by user. Results up to this point have been saved.\")\n",
    "        logger.info(\"You can resume the process by running the script again.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "def analyze_error_cases(df, classifier, n_examples=10):\n",
    "    \"\"\"\n",
    "    Analyze error cases to understand where the model fails\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with test data\n",
    "        classifier: Trained classifier\n",
    "        n_examples: Number of error examples to analyze\n",
    "    \"\"\"\n",
    "    logger.info(\"Analyzing error cases...\")\n",
    "    \n",
    "    # Convert labels_list if needed\n",
    "    if isinstance(df['labels_list'].iloc[0], str):\n",
    "        df['labels_list'] = df['labels_list'].str.strip('[]').str.split(',')\n",
    "        df['labels_list'] = df['labels_list'].apply(lambda x: [item.strip().strip(\"'\\\"\") for item in x])\n",
    "    \n",
    "    # Split data\n",
    "    _, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Find error cases\n",
    "    error_cases = []\n",
    "    \n",
    "    try:\n",
    "        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Finding errors\"):\n",
    "            try:\n",
    "                text = row['survey_answer']\n",
    "                true_labels = set(row['labels_list'])\n",
    "                \n",
    "                # Get predictions with detailed info\n",
    "                prediction = classifier.predict(text, return_scores=True)\n",
    "                pred_labels = set(prediction['labels'])\n",
    "                \n",
    "                # Check if there's an error\n",
    "                if true_labels != pred_labels:\n",
    "                    missed_labels = true_labels - pred_labels\n",
    "                    extra_labels = pred_labels - true_labels\n",
    "                    \n",
    "                    error_cases.append({\n",
    "                        'text': text,\n",
    "                        'true_labels': list(true_labels),  # Convert sets to lists for JSON serialization\n",
    "                        'pred_labels': list(pred_labels),\n",
    "                        'missed_labels': list(missed_labels),\n",
    "                        'extra_labels': list(extra_labels),\n",
    "                        'tfidf_scores': prediction['tfidf_scores'],\n",
    "                        'bert_scores': prediction['bert_scores'],\n",
    "                        'combined_scores': dict(prediction['sorted_scores'])\n",
    "                    })\n",
    "                    \n",
    "                    # Save partial results periodically\n",
    "                    if len(error_cases) % 50 == 0:\n",
    "                        with open('error_analysis_partial.json', 'w') as f:\n",
    "                            json.dump({'error_cases': error_cases[:n_examples]}, f, indent=2)\n",
    "            except KeyboardInterrupt:\n",
    "                logger.warning(\"KeyboardInterrupt during error analysis. Processing collected cases...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error analyzing sample {idx}: {e}\")\n",
    "                continue\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"KeyboardInterrupt during error analysis. Processing collected cases...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during error analysis: {e}\")\n",
    "    \n",
    "    logger.info(f\"Found {len(error_cases)} error cases out of {len(test_df)} test examples\")\n",
    "    \n",
    "    # If we don't have any error cases, return early\n",
    "    if not error_cases:\n",
    "        logger.warning(\"No error cases found for analysis.\")\n",
    "        return {\n",
    "            'missed_labels': {},\n",
    "            'extra_labels': {},\n",
    "            'error_cases': []\n",
    "        }\n",
    "    \n",
    "    # Analyze which labels are most frequently missed\n",
    "    missed_label_counts = {}\n",
    "    extra_label_counts = {}\n",
    "    \n",
    "    for case in error_cases:\n",
    "        for label in case['missed_labels']:\n",
    "            missed_label_counts[label] = missed_label_counts.get(label, 0) + 1\n",
    "        \n",
    "        for label in case['extra_labels']:\n",
    "            extra_label_counts[label] = extra_label_counts.get(label, 0) + 1\n",
    "    \n",
    "    # Sort by count\n",
    "    missed_label_counts = {k: v for k, v in sorted(missed_label_counts.items(), \n",
    "                                                   key=lambda item: item[1], \n",
    "                                                   reverse=True)}\n",
    "    extra_label_counts = {k: v for k, v in sorted(extra_label_counts.items(), \n",
    "                                                  key=lambda item: item[1], \n",
    "                                                  reverse=True)}\n",
    "    \n",
    "    logger.info(\"Most frequently missed labels:\")\n",
    "    for label, count in list(missed_label_counts.items())[:5]:\n",
    "        logger.info(f\"  {label}: {count} times\")\n",
    "    \n",
    "    logger.info(\"Most frequently incorrectly added labels:\")\n",
    "    for label, count in list(extra_label_counts.items())[:5]:\n",
    "        logger.info(f\"  {label}: {count} times\")\n",
    "    \n",
    "    # Save detailed analysis of top N error cases\n",
    "    with open('error_analysis.txt', 'w') as f:\n",
    "        f.write(\"DETAILED ERROR ANALYSIS\\n\")\n",
    "        f.write(\"======================\\n\\n\")\n",
    "        \n",
    "        f.write(\"Most frequently missed labels:\\n\")\n",
    "        for label, count in missed_label_counts.items():\n",
    "            f.write(f\"  {label}: {count} times\\n\")\n",
    "        \n",
    "        f.write(\"\\nMost frequently incorrectly added labels:\\n\")\n",
    "        for label, count in extra_label_counts.items():\n",
    "            f.write(f\"  {label}: {count} times\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nSAMPLE ERROR CASES\\n\")\n",
    "        f.write(\"==================\\n\\n\")\n",
    "        \n",
    "        for i, case in enumerate(error_cases[:n_examples]):\n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"Text: {case['text']}\\n\")\n",
    "            f.write(f\"True labels: {', '.join(case['true_labels'])}\\n\")\n",
    "            f.write(f\"Predicted labels: {', '.join(case['pred_labels'])}\\n\")\n",
    "            f.write(f\"Missed labels: {', '.join(case['missed_labels'])}\\n\")\n",
    "            f.write(f\"Extra labels: {', '.join(case['extra_labels'])}\\n\")\n",
    "            \n",
    "            f.write(\"\\nTF-IDF scores:\\n\")\n",
    "            for label, score in sorted(case['tfidf_scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "                f.write(f\"  {label}: {score:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\nBERT scores:\\n\")\n",
    "            for label, score in sorted(case['bert_scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "                f.write(f\"  {label}: {score:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\nCombined scores:\\n\")\n",
    "            for label, score in sorted(case['combined_scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "                f.write(f\"  {label}: {score:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    # Also save as JSON for easier programmatic analysis\n",
    "    with open('error_analysis.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'missed_labels': missed_label_counts,\n",
    "            'extra_labels': extra_label_counts,\n",
    "            'error_cases': error_cases[:n_examples]\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Error analysis saved to 'error_analysis.txt' and 'error_analysis.json'\")\n",
    "    \n",
    "    return {\n",
    "        'missed_labels': missed_label_counts,\n",
    "        'extra_labels': extra_label_counts,\n",
    "        'error_cases': error_cases[:n_examples]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8512d",
   "metadata": {},
   "source": [
    "### BERT Zero-Shot Classifier with Optimizatio nCode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89af843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 19:57:28,238 - INFO - Initializing model facebook/bart-large-mnli on device -1\n",
      "Device set to use cpu\n",
      "2025-04-12 19:57:29,504 - INFO - Model loaded in 1.27 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Individual predictions:\n",
      "\n",
      "Text: 'I love hiking in the mountains and trying local foods wherever I travel.'\n",
      "Prediction time: 3.1845 seconds\n",
      "  Travel: 0.9361\n",
      "  Food: 0.7752\n",
      "  Arts: 0.0063\n",
      "  Music: 0.0013\n",
      "  Sports: 0.0011\n",
      "  Education: 0.0009\n",
      "  Technology: 0.0004\n",
      "\n",
      "Text: 'I'm a software developer who plays guitar in a band on weekends.'\n",
      "Prediction time: 2.1096 seconds\n",
      "  Technology: 0.9120\n",
      "  Music: 0.3961\n",
      "  Arts: 0.0085\n",
      "  Travel: 0.0048\n",
      "  Sports: 0.0014\n",
      "  Education: 0.0006\n",
      "  Food: 0.0003\n",
      "\n",
      "Batch predictions:\n",
      "Batch prediction time for 5 texts: 5.4421 seconds\n",
      "Average time per text: 1.0884 seconds\n",
      "\n",
      "Sample result for: 'I love hiking in the mountains and trying local foods wherever I travel.'\n",
      "  Travel: 0.9361\n",
      "  Food: 0.7752\n",
      "  Arts: 0.0063\n",
      "  Music: 0.0013\n",
      "  Sports: 0.0011\n",
      "  Education: 0.0009\n",
      "  Technology: 0.0004\n",
      "\n",
      "Optimizer stats:\n",
      "  disk_cache_hits: 2\n",
      "  disk_cache_misses: 5\n",
      "  total_predictions: 7\n",
      "  batch_predictions: 1\n",
      "  memory_cache_hits: 0\n",
      "  memory_cache_misses: 5\n",
      "  memory_cache_size: 2048\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict, Any, Union, Optional\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OptimizedBERTZeroShotClassifier:\n",
    "    \"\"\"\n",
    "    Optimized BERT zero-shot classifier with caching and performance improvements\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model_name: str = 'facebook/bart-large-mnli',\n",
    "                 use_gpu: bool = torch.cuda.is_available(),\n",
    "                 cache_size: int = 1024,\n",
    "                 batch_size: int = 8):\n",
    "        \"\"\"\n",
    "        Initialize the optimized BERT zero-shot classifier\n",
    "        \n",
    "        Args:\n",
    "            model_name: The name of the pre-trained model to use\n",
    "            use_gpu: Whether to use GPU for inference\n",
    "            cache_size: Size of the LRU cache for classification results\n",
    "            batch_size: Batch size for batch processing\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.use_gpu = use_gpu\n",
    "        self.cache_size = cache_size\n",
    "        self.batch_size = batch_size\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "        # Initialize the model\n",
    "        self._init_model()\n",
    "        \n",
    "        # Set up LRU cache for the classification method\n",
    "        # Note: This properly caches the _classify_uncached method with self as first argument\n",
    "        self._classify_cached = lru_cache(maxsize=cache_size)(self._classify_uncached)\n",
    "    \n",
    "    def _init_model(self):\n",
    "        \"\"\"Initialize the BERT model with optimizations\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Set device\n",
    "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
    "            logger.info(f\"Initializing model {self.model_name} on device {device}\")\n",
    "            \n",
    "            # Initialize tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
    "            \n",
    "            # Move model to device\n",
    "            if device >= 0:\n",
    "                self.model.to(f\"cuda:{device}\")\n",
    "            \n",
    "            # Create zero-shot pipeline\n",
    "            self.classifier = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # For batch processing\n",
    "            if self.use_gpu:\n",
    "                # Enable optimizations for GPU\n",
    "                self.model = self.model.half()  # Use half precision\n",
    "                logger.info(\"Using half precision for GPU\")\n",
    "            \n",
    "            logger.info(f\"Model loaded in {time.time() - start_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _classify_uncached(self, text: str, categories_tuple: tuple, multi_label: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Uncached version of the classification method\n",
    "        \n",
    "        Args:\n",
    "            text: The text to classify\n",
    "            categories_tuple: Tuple of category labels (for cache hashability)\n",
    "            multi_label: Whether to use multi-label classification\n",
    "            \n",
    "        Returns:\n",
    "            Classification results\n",
    "        \"\"\"\n",
    "        # Convert tuple back to list for the classifier\n",
    "        categories_list = list(categories_tuple)\n",
    "        \n",
    "        # Record time for performance monitoring\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Call the classifier\n",
    "        result = self.classifier(text, categories_list, multi_label=multi_label)\n",
    "        \n",
    "        # Add timing information\n",
    "        result['processing_time'] = time.time() - start_time\n",
    "        \n",
    "        self.cache_misses += 1\n",
    "        return result\n",
    "    \n",
    "    def classify_text(self, text: str, categories: List[str], multi_label: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Classify text using the zero-shot classifier with caching\n",
    "        \n",
    "        Args:\n",
    "            text: The text to classify\n",
    "            categories: List of category labels\n",
    "            multi_label: Whether to use multi-label classification\n",
    "            \n",
    "        Returns:\n",
    "            Classification results\n",
    "        \"\"\"\n",
    "        # Convert categories to tuple for hashability in cache\n",
    "        categories_tuple = tuple(categories)\n",
    "        \n",
    "        # Check if it's in cache\n",
    "        before_cache_misses = self.cache_misses\n",
    "        \n",
    "        # Use the cached version of classify_uncached\n",
    "        result = self._classify_cached(text, categories_tuple, multi_label)\n",
    "        \n",
    "        # Check if it was a cache hit\n",
    "        if self.cache_misses == before_cache_misses:\n",
    "            self.cache_hits += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_classify(self, texts: List[str], categories: List[str], multi_label: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Classify multiple texts in batch for better performance\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to classify\n",
    "            categories: List of category labels\n",
    "            multi_label: Whether to use multi-label classification\n",
    "            \n",
    "        Returns:\n",
    "            List of classification results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert categories to tuple once (for cache hashability)\n",
    "        categories_tuple = tuple(categories)\n",
    "        \n",
    "        # Process all texts\n",
    "        for text in texts:\n",
    "            # Get cached or new classification\n",
    "            result = self.classify_text(text, categories, multi_label)\n",
    "            results.append(result)\n",
    "        \n",
    "        logger.info(f\"Batch processed {len(texts)} texts in {time.time() - start_time:.2f} seconds\")\n",
    "        logger.info(f\"Cache hits: {self.cache_hits}, misses: {self.cache_misses}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        return {\n",
    "            'hits': self.cache_hits,\n",
    "            'misses': self.cache_misses,\n",
    "            'size': self.cache_size,\n",
    "            'current_usage': len(self._classify_cached.cache_info().currsize) if hasattr(self._classify_cached, 'cache_info') else 0\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the classification cache\"\"\"\n",
    "        if hasattr(self._classify_cached, 'cache_clear'):\n",
    "            self._classify_cached.cache_clear()\n",
    "            logger.info(\"Cache cleared\")\n",
    "\n",
    "\n",
    "class HybridModelBERTOptimizer:\n",
    "    \"\"\"\n",
    "    Class to optimize BERT usage in the hybrid interest classifier model\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 bert_model_name: str = 'facebook/bart-large-mnli',\n",
    "                 cache_dir: Optional[str] = './bert_cache',\n",
    "                 use_gpu: bool = torch.cuda.is_available(),\n",
    "                 cache_size: int = 2048,\n",
    "                 batch_size: int = 16):\n",
    "        \"\"\"\n",
    "        Initialize the BERT optimizer for hybrid model\n",
    "        \n",
    "        Args:\n",
    "            bert_model_name: Name of the BERT model to use\n",
    "            cache_dir: Directory to cache preprocessed BERT features\n",
    "            use_gpu: Whether to use GPU\n",
    "            cache_size: Size of the LRU cache\n",
    "            batch_size: Batch size for processing\n",
    "        \"\"\"\n",
    "        self.bert_model_name = bert_model_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.use_gpu = use_gpu\n",
    "        self.cache_size = cache_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Create cache directory if needed\n",
    "        if cache_dir and not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        \n",
    "        # Initialize optimized classifier\n",
    "        self.bert_classifier = OptimizedBERTZeroShotClassifier(\n",
    "            model_name=bert_model_name,\n",
    "            use_gpu=use_gpu,\n",
    "            cache_size=cache_size,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Track statistics\n",
    "        self.stats = {\n",
    "            'disk_cache_hits': 0,\n",
    "            'disk_cache_misses': 0,\n",
    "            'total_predictions': 0,\n",
    "            'batch_predictions': 0\n",
    "        }\n",
    "    \n",
    "    def _get_cache_path(self, text: str, categories: List[str]) -> str:\n",
    "        \"\"\"Get path for disk cache file\"\"\"\n",
    "        import hashlib\n",
    "        \n",
    "        # Create a hash of the text and categories\n",
    "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        categories_str = '_'.join(sorted(categories))\n",
    "        categories_hash = hashlib.md5(categories_str.encode()).hexdigest()\n",
    "        \n",
    "        return os.path.join(self.cache_dir, f\"{text_hash}_{categories_hash}.pkl\")\n",
    "    \n",
    "    def get_bert_predictions(self, text: str, categories: List[str], use_disk_cache: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get BERT predictions with optimizations\n",
    "        \n",
    "        Args:\n",
    "            text: Text to classify\n",
    "            categories: Interest categories\n",
    "            use_disk_cache: Whether to use disk cache\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping categories to scores\n",
    "        \"\"\"\n",
    "        self.stats['total_predictions'] += 1\n",
    "        \n",
    "        # Check disk cache first if enabled\n",
    "        if use_disk_cache and self.cache_dir:\n",
    "            cache_path = self._get_cache_path(text, categories)\n",
    "            \n",
    "            if os.path.exists(cache_path):\n",
    "                try:\n",
    "                    with open(cache_path, 'rb') as f:\n",
    "                        cached_result = pickle.load(f)\n",
    "                    \n",
    "                    self.stats['disk_cache_hits'] += 1\n",
    "                    return cached_result\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error loading from disk cache: {e}\")\n",
    "        \n",
    "        if use_disk_cache:\n",
    "            self.stats['disk_cache_misses'] += 1\n",
    "        \n",
    "        # Get prediction from BERT classifier\n",
    "        result = self.bert_classifier.classify_text(text, categories, multi_label=True)\n",
    "        \n",
    "        # Convert to simple dictionary format\n",
    "        scores = dict(zip(result['labels'], result['scores']))\n",
    "        \n",
    "        # Save to disk cache if enabled\n",
    "        if use_disk_cache and self.cache_dir:\n",
    "            cache_path = self._get_cache_path(text, categories)\n",
    "            try:\n",
    "                with open(cache_path, 'wb') as f:\n",
    "                    pickle.dump(scores, f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error saving to disk cache: {e}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def batch_get_bert_predictions(self, texts: List[str], categories: List[str], use_disk_cache: bool = True) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Get BERT predictions for multiple texts with optimizations\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to classify\n",
    "            categories: Interest categories\n",
    "            use_disk_cache: Whether to use disk cache\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping texts to prediction dictionaries\n",
    "        \"\"\"\n",
    "        self.stats['batch_predictions'] += 1\n",
    "        self.stats['total_predictions'] += len(texts)\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        results = {}\n",
    "        \n",
    "        # Check disk cache first for each text\n",
    "        uncached_texts = []\n",
    "        \n",
    "        if use_disk_cache and self.cache_dir:\n",
    "            for text in texts:\n",
    "                cache_path = self._get_cache_path(text, categories)\n",
    "                \n",
    "                if os.path.exists(cache_path):\n",
    "                    try:\n",
    "                        with open(cache_path, 'rb') as f:\n",
    "                            results[text] = pickle.load(f)\n",
    "                        self.stats['disk_cache_hits'] += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error loading from disk cache: {e}\")\n",
    "                        uncached_texts.append(text)\n",
    "                else:\n",
    "                    uncached_texts.append(text)\n",
    "                    self.stats['disk_cache_misses'] += 1\n",
    "        else:\n",
    "            uncached_texts = texts\n",
    "        \n",
    "        # Process uncached texts in smaller batches\n",
    "        if uncached_texts:\n",
    "            # Process in batches to avoid memory issues\n",
    "            for i in range(0, len(uncached_texts), self.batch_size):\n",
    "                batch_texts = uncached_texts[i:i+self.batch_size]\n",
    "                \n",
    "                # Get predictions for this batch\n",
    "                for text in batch_texts:\n",
    "                    result = self.bert_classifier.classify_text(text, categories, multi_label=True)\n",
    "                    \n",
    "                    # Convert to simple dictionary format\n",
    "                    scores = dict(zip(result['labels'], result['scores']))\n",
    "                    results[text] = scores\n",
    "                    \n",
    "                    # Save to disk cache if enabled\n",
    "                    if use_disk_cache and self.cache_dir:\n",
    "                        cache_path = self._get_cache_path(text, categories)\n",
    "                        try:\n",
    "                            with open(cache_path, 'wb') as f:\n",
    "                                pickle.dump(scores, f)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Error saving to disk cache: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get optimizer statistics\"\"\"\n",
    "        # Add in-memory cache stats\n",
    "        bert_cache_stats = {\n",
    "            'memory_cache_hits': self.bert_classifier.cache_hits,\n",
    "            'memory_cache_misses': self.bert_classifier.cache_misses,\n",
    "            'memory_cache_size': self.bert_classifier.cache_size\n",
    "        }\n",
    "        \n",
    "        return {**self.stats, **bert_cache_stats}\n",
    "    \n",
    "    def clear_caches(self):\n",
    "        \"\"\"Clear all caches\"\"\"\n",
    "        # Clear in-memory cache\n",
    "        self.bert_classifier.clear_cache()\n",
    "        \n",
    "        # Clear disk cache if enabled\n",
    "        if self.cache_dir and os.path.exists(self.cache_dir):\n",
    "            import glob\n",
    "            cache_files = glob.glob(os.path.join(self.cache_dir, \"*.pkl\"))\n",
    "            for file in cache_files:\n",
    "                try:\n",
    "                    os.remove(file)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error removing cache file {file}: {e}\")\n",
    "            \n",
    "            logger.info(f\"Cleared {len(cache_files)} disk cache files\")\n",
    "\n",
    "\n",
    "# Function to optimize BERT predictions for the hybrid model\n",
    "def optimize_bert_for_hybrid_model(texts: List[str], categories: List[str]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Optimize BERT predictions for use in a hybrid model\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to classify\n",
    "        categories: List of interest categories\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping text to BERT predictions\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optimizer = HybridModelBERTOptimizer(\n",
    "        bert_model_name='facebook/bart-large-mnli',\n",
    "        cache_dir='./bert_cache',\n",
    "        use_gpu=torch.cuda.is_available(),\n",
    "        cache_size=2048,\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Get batch predictions\n",
    "    return optimizer.batch_get_bert_predictions(texts, categories)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    \"\"\"Example usage of the optimized BERT classifier\"\"\"\n",
    "    # Example categories\n",
    "    categories = [\"Music\", \"Food\", \"Sports\", \"Technology\", \"Arts\", \"Travel\", \"Education\"]\n",
    "    \n",
    "    # Example texts\n",
    "    example_texts = [\n",
    "        \"I love hiking in the mountains and trying local foods wherever I travel.\",\n",
    "        \"I'm a software developer who plays guitar in a band on weekends.\",\n",
    "        \"I spend most of my time reading books and attending online courses.\",\n",
    "        \"I enjoy painting landscapes and visiting art museums when I travel.\",\n",
    "        \"I'm passionate about fitness and healthy cooking.\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = HybridModelBERTOptimizer(cache_dir=\"./bert_cache\")\n",
    "    \n",
    "    # Individual predictions\n",
    "    print(\"\\nIndividual predictions:\")\n",
    "    for text in example_texts[:2]:  # Just do 2 for demonstration\n",
    "        print(f\"\\nText: '{text}'\")\n",
    "        start_time = time.time()\n",
    "        result = optimizer.get_bert_predictions(text, categories)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"Prediction time: {elapsed:.4f} seconds\")\n",
    "        for category, score in sorted(result.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {category}: {score:.4f}\")\n",
    "    \n",
    "    # Batch predictions\n",
    "    print(\"\\nBatch predictions:\")\n",
    "    start_time = time.time()\n",
    "    batch_results = optimizer.batch_get_bert_predictions(example_texts, categories)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"Batch prediction time for {len(example_texts)} texts: {elapsed:.4f} seconds\")\n",
    "    print(f\"Average time per text: {elapsed/len(example_texts):.4f} seconds\")\n",
    "    \n",
    "    # Show a sample result\n",
    "    sample_text = example_texts[0]\n",
    "    print(f\"\\nSample result for: '{sample_text}'\")\n",
    "    for category, score in sorted(batch_results[sample_text].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {category}: {score:.4f}\")\n",
    "    \n",
    "    # Show stats\n",
    "    print(\"\\nOptimizer stats:\")\n",
    "    stats = optimizer.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
